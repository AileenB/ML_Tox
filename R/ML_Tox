# 
#  ML_Tox.r
#  
#  Copyright 2019 Aileen Bahl
# 
#  This program is free software; you can redistribute it and/or
#  modify it under the terms of the GNU General Public License
#  as published by the Free Software Foundation; either version 3
#  of the License, or (at your option) any later version.
#  
#  This program is distributed in the hope that it will be useful,
#  but WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#  GNU General Public License for more details.
#  
#  You should have received a copy of the GNU General Public License
#  along with this program; if not, write to the Free Software
#  Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301, USA.
# 

# Unsupervised (Principle component analysis and k-nearest neighbors) and supervised (Random forest with recursive feature elimination) machine learning for toxicity prediction and input parameter ranking.
#
#' @param file            File name of the input file of type .xlsx or .csv. The first column of the file should hold name of compounds to be used in the classification (e.g. chemical or nanomaterial names), followed by one column per descriptor and a last column called 'Toxicity' holding the labels (e.g. 'active' and 'passive').
#' @param sheetInd        Index or name of the sheet of the input file that holds the relevant data. 
#' @param classLabelPos   The name of the positive labels. Defaults to 'active'.
#' @param classLabelNeg   The name of the negative labels. Defaults to 'passive'.
#' @param pca             Should principle component analysis be performed? 
#' @param knn             Should k-nearest neighbors approach be performed?
#' @param k               Number of neighbors to be considered for 'read-across'. Defaults to 1.
#' @param usePC           Which principle components to use to define similarity between compounds as a basis for the k-nearest neighbors approach? Defaults to PC1 and 2.
#' @param rf              Should Random forest be performed?
#' @param ntree           Number of trees to build in the Random forest approach. Higher numbers of trees improved the stability of the Random forest but increase the runtime. Defaults to 5000.
#' @param rfe             Should backward recursive feature elimination be performed?
#' @param importance      Which importance measure should recursive feature elimination be based on? Defaults to "MDA" (mean decrease of accuracy). The alternative is "Gini" (Gini importance).


#' @example
#'   ML_Tox("./testData.xlsx", sheetInd=1, classLabelPos="active", classLabelNeg="passive", pca=TRUE, knn=TRUE, k=1, usePC=c(1:2), rf=TRUE, ntree=5000, rfe=TRUE, importance="MDA")
#'





require(tools)
require(xlsx)
require(class)
require(randomForest)


ML_Tox <- function(file, sheetInd=1, classLabelPos="active", classLabelNeg="passive", pca=TRUE, knn=TRUE, k=1, usePC=c(1:2), rf=TRUE, ntree=5000, rfe=TRUE, importance="MDA"){
  
  # read input file

  if(file_ext(file) == "xlsx"){
    myData <- read.xlsx2(file, sheetInd)
    myData <- myData[which(myData[,1] != ""),]
    myData <- myData[,c(1:which(colnames(myData) == "Toxicity"))]
  } else if (file_ext(file) == "csv"){
    myData <- read.csv(file)
    myData <- myData[which(myData[,1] != ""),]
    myData <- myData[,c(1:which(colnames(myData) == "Toxicity"))]
  } else{
    print("Error: Please provide input as .xlsx or .csv")
  }
  
  myDescriptors <- myData[-c(1,dim(myData)[2])]
  rownames(myDescriptors) <- myData[,1]
  myDescriptors_numeric <- apply(myDescriptors,2,as.numeric)
  myTox <- myData[,dim(myData)[2]]
  myTox <- droplevels(myTox,"")
  
  # perform pca
  
  if(pca==T){
    myPCAResult <- prcomp(myDescriptors_numeric, scale = T)
  }
  
  # perform knn
  
  if(knn==T){
    
    myChosenPCs <- myPCAResult$x[,usePC]
    
    predictKNN <- function(i){
      myKNN <- knn(myChosenPCs[-i,],myChosenPCs[i,], cl=myTox[-i], k=k, prob=T)
      return(myKNN)
    }
  
    myPredictions <- sapply(c(1:dim(myChosenPCs)[1]),predictKNN)
  
    myComparisonOfClasses <- data.frame(PredictedClass=myPredictions,TrueClass=myTox)
    rownames(myComparisonOfClasses) <- rownames(myDescriptors)
    print("PCA + knn:")
    print(myComparisonOfClasses)
    
    myCorrectPrediction <- sum(myPredictions==myTox)
    print("Number of correct predictions:")
    print(myCorrectPrediction)
    
    mySensitivity <- sum(myPredictions == classLabelPos & myTox == classLabelPos) / sum(myPredictions == classLabelPos)
    print("Sensitivity:")
    print(mySensitivity)
    
    mySpecificity <- sum(myPredictions == classLabelNeg & myTox == classLabelNeg) / sum(myPredictions == classLabelNeg)
    print("Specificity:")
    print(mySpecificity)
    
    myBalancedAccuracy <- (mySensitivity + mySpecificity)/2 
    print("Balanced accuracy:")
    print(myBalancedAccuracy)
    cat("\n")
    
  }
  
  # perform rf
  
  if(rf==T){
    
    myDescriptors_numeric <- as.data.frame(myDescriptors_numeric)
    rownames(myDescriptors_numeric) <- myData[,1]
    
    predictRF <- function(i){
      myRF <- randomForest(myDescriptors_numeric[-c(i),],myTox[-c(i)], myDescriptors_numeric[c(i),],myTox[c(i)], ntree=ntree, importance=T)
      myImportance <- round(importance(myRF)[,4], 2)
      myPredictedClass <- myRF$test$predicted
      myList=list(prediction=myPredictedClass,importance=myImportance)
      return(myList)
    }
    
    myPredictions <- sapply(c(1:dim(myDescriptors_numeric)[1]),predictRF)
    
    myComparisonOfClasses <- data.frame(PredictedClass=unlist(myPredictions[1,]),TrueClass=myTox)
    rownames(myComparisonOfClasses) <- rownames(myDescriptors_numeric)
    print("RF - full model:")
    
    myImportanceMatrix <- do.call(cbind, myPredictions[2,])
    myParameterImportance <- sort(rowMeans(myImportanceMatrix))
    
    print(myParameterImportance)
    print(myComparisonOfClasses)
    
    myCorrectPrediction <- sum(unlist(myPredictions[1,])==myTox)
    print("Number of correct predictions:")
    print(myCorrectPrediction)
    
    mySensitivity <- sum(unlist(myPredictions[1,]) == classLabelPos & myTox == classLabelPos) / sum(unlist(myPredictions[1,]) == classLabelPos)
    print("Sensitivity:")
    print(mySensitivity)
    
    mySpecificity <- sum(unlist(myPredictions[1,]) == classLabelNeg & myTox == classLabelNeg) / sum(unlist(myPredictions[1,]) == classLabelNeg)
    print("Specificity:")
    print(mySpecificity)
    
    myBalancedAccuracy <- (mySensitivity + mySpecificity)/2 
    print("Balanced accuracy:")
    print(myBalancedAccuracy)
    cat("\n")
    
    if(rfe == T){
    
      minValue <- names(myParameterImportance[1])
      a <- which(colnames(myDescriptors_numeric)==minValue)
      
      print("RF - reduced models")
    
      while(length(a) < (dim(myDescriptors)[2]-1)){
      
        predictRF <- function(i){
          myRF <- randomForest(myDescriptors_numeric[,-c(a)][-c(i),],myTox[-c(i)], myDescriptors_numeric[,-c(a)][c(i),],myTox[c(i)], ntree=ntree, importance=T)
          if (importance=="Gini"){
            myImportance <- round(importance(myRF)[,4], 2)
          } else if (importance=="MDA"){
            myImportance <- round(importance(myRF)[,3], 2)
          }
          myPredictedClass <- myRF$test$predicted
          myList=list(prediction=myPredictedClass,importance=myImportance)
          return(myList)
        }
        
        myPredictions <- sapply(c(1:dim(myDescriptors_numeric)[1]),predictRF)
        
        myComparisonOfClasses <- data.frame(PredictedClass=unlist(myPredictions[1,]),TrueClass=myTox)
        rownames(myComparisonOfClasses) <- rownames(myDescriptors_numeric)
        
        myImportanceMatrix <- do.call(cbind, myPredictions[2,])
        myParameterImportance <- sort(rowMeans(myImportanceMatrix))
        
        print(myParameterImportance)
        print(myComparisonOfClasses)
        
        myCorrectPrediction <- sum(myPredictions==myTox)
        print("Number of correct predictions:")
        print(myCorrectPrediction)
        
        mySensitivity <- sum(unlist(myPredictions[1,]) == classLabelPos & myTox == classLabelPos) / sum(unlist(myPredictions[1,]) == classLabelPos)
        print("Sensitivity:")
        print(mySensitivity)
        
        mySpecificity <- sum(unlist(myPredictions[1,]) == classLabelNeg & myTox == classLabelNeg) / sum(unlist(myPredictions[1,]) == classLabelNeg)
        print("Specificity:")
        print(mySpecificity)
        
        myBalancedAccuracy <- (mySensitivity + mySpecificity)/2 
        print("Balanced accuracy:")
        print(myBalancedAccuracy)
        cat("\n")
      
        minValue <- names(sort(rowMeans(myImportanceMatrix))[1])
        a_tmp <- which(colnames(myDescriptors_numeric)==minValue)
        a <- c(a,a_tmp)
    
        
        }
    }    
  }
}



